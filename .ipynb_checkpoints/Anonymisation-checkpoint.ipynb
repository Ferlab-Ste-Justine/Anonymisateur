{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ba12068-e999-4033-9c41-02c9b75084ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import sys\n",
    "\n",
    "sys.path.append('/home/acantin/.local/lib/python3.10/site-packages')\n",
    "\n",
    "import numpy\n",
    "import transformers\n",
    "import tqdm\n",
    "#import deep-translator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7219871a-b3eb-4b44-8391-6d3e88d5e690",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f61a72b-15d7-4586-9acd-f40079f4542b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/acantin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from deep_translator import GoogleTranslator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from transformers import pipeline\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pickle\n",
    "\n",
    "# Ensure nltk punkt is downloaded\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2b6d64-675c-470e-9381-0192c9814117",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Traduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8e28b99-275d-40ce-b3ed-405f29508ec0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef transform_caps_word(word):\\n    if word.isupper():\\n        return word[0] + word[1:].lower()\\n    return word\\n\\ndef transform_text(text):\\n    words = text.split()\\n    transformed_words = [transform_caps_word(word) for word in words]\\n    return ' '.join(transformed_words)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fonction pour diviser un texte en phrases\n",
    "def split_text_into_sentences(text):\n",
    "    \"\"\"\n",
    "    Divise le texte en phrases en utilisant NLTK.\n",
    "    \n",
    "    Args:\n",
    "    text (str): Le texte à diviser en phrases.\n",
    "    \n",
    "    Returns:\n",
    "    list: Une liste contenant les phrases du texte.\n",
    "    \"\"\"\n",
    "    return nltk.sent_tokenize(text)\n",
    "\n",
    "# Fonction de traduction avec Google Translate (deep_translator)\n",
    "# Cette fonction prend un texte dans une langue source (par défaut 'en' pour anglais)\n",
    "# et le traduit dans une langue de destination (par défaut 'fr' pour français)\n",
    "def translate_text_v2(text, src='en', dest='fr'):\n",
    "    \"\"\"\n",
    "    Traduit un texte en utilisant GoogleTranslator.\n",
    "    La traduction se fait phrase par phrase pour éviter les limites de caractères.\n",
    "\n",
    "    Args:\n",
    "    text (str): Le texte à traduire.\n",
    "    src (str): La langue source (par défaut 'en').\n",
    "    dest (str): La langue de destination (par défaut 'fr').\n",
    "\n",
    "    Returns:\n",
    "    str: Le texte traduit.\n",
    "    \"\"\"\n",
    "    # Diviser le texte en phrases pour une meilleure gestion\n",
    "    sentences = split_text_into_sentences(text)\n",
    "    translated_sentences = []   # Liste pour stocker les phrases traduites\n",
    "\n",
    "    current_chunk = \"\"          # Variable pour stocker les morceaux de texte à traduire\n",
    "    translator = GoogleTranslator(source=src, target=dest)\n",
    "    count = 0                   # Compteur pour contrôler la première phrase\n",
    "    for sentence in sentences:\n",
    "        # Pour la première phrase(souvent rempli d<acronyme et ID), on vérifie si la traduction est significative\n",
    "        if count == 0:\n",
    "            # Traduction\n",
    "            x = GoogleTranslator(source='fr', target='en').translate(sentence)\n",
    "            # Si la traduction est trop courte(indicateur demauvaise traduction), garder la phrase originale\n",
    "            if len(x) < 2*len(sentence)//3:\n",
    "                translated_sentences.append(sentence)\n",
    "                current_chunk = sentence + \" \"\n",
    "            else:\n",
    "                translated_sentences.append(x)\n",
    "        \n",
    "        # Si le texte traduit est trop long pour être ajouté à l'actuel 'chunk'        \n",
    "        elif len(current_chunk) + len(sentence) + 1 <= 5000:\n",
    "            current_chunk += sentence + \" \"\n",
    "            \n",
    "        else:\n",
    "            # Traduire le 'chunk' et recommencer\n",
    "            x = GoogleTranslator(source='fr', target='en').translate(current_chunk)\n",
    "            translated_sentences.append(x)\n",
    "            current_chunk = sentence + \" \"\n",
    "            \n",
    "        count += 1\n",
    "    \n",
    "    # Traduire le dernier 'chunk' s'il en reste\n",
    "    if current_chunk:\n",
    "        translated_sentences.append(GoogleTranslator(source='fr', target='en').translate(current_chunk))\n",
    "\n",
    "    # Retourner toutes les phrases traduites sous forme de texte\n",
    "    return ' '.join(translated_sentences)\n",
    "\n",
    "# Fonction pour diviser une étiquette en mots et les convertir en minuscules\n",
    "def split_and_lowercase(label):\n",
    "    \"\"\"\n",
    "    Divise l'étiquette en mots et les convertit en minuscules.\n",
    "    \n",
    "    Args:\n",
    "    label (str): L'étiquette à traiter.\n",
    "    \n",
    "    Returns:\n",
    "    list: Une liste de mots en minuscules.\n",
    "    \"\"\"\n",
    "    return re.findall(r'\\b\\w+\\b', label.lower())\n",
    "\n",
    "# Fonction pour transformer un texte\n",
    "# Cette fonction passe tous les mots en minuscules sauf la première lettre du premier mot\n",
    "def transform_text(text):\n",
    "    \"\"\"\n",
    "    Transforme les mots en majuscules en minuscules sauf la première lettre du premier mot.\n",
    "    \n",
    "    Args:\n",
    "    text (str): Le texte à transformer.\n",
    "    \n",
    "    Returns:\n",
    "    str: Le texte transformé.\n",
    "    \"\"\"\n",
    "    # Séparer le texte en mots\n",
    "    words = text.split()\n",
    "    \n",
    "    # Transformer les mots tout en respectant la casse pour le premier caractère\n",
    "    return ' '.join([word[0] + word[1:].lower() if word.isupper() else word for word in words])\n",
    "\n",
    "\"\"\"\n",
    "def transform_caps_word(word):\n",
    "    if word.isupper():\n",
    "        return word[0] + word[1:].lower()\n",
    "    return word\n",
    "\n",
    "def transform_text(text):\n",
    "    words = text.split()\n",
    "    transformed_words = [transform_caps_word(word) for word in words]\n",
    "    return ' '.join(transformed_words)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86d8abd-989e-4584-90d9-276de0c50066",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7971fba-d3f1-4d79-b7bf-10e1fc13bee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fonction du modèle\n",
    "def NER_text_split_v2(text, ner_pipeline):\n",
    "    \"\"\"\n",
    "    Applique le modèle NER sur un texte divisé en phrases.\n",
    "\n",
    "    Paramètres:\n",
    "    text (str): Le texte d'entrée à traiter.\n",
    "    ner_pipeline (pipeline): Le pipeline NER (modèle NER utilisé pour la reconnaissance d'entités nommées).\n",
    "\n",
    "    Retourne:\n",
    "    list: Les labels NER (entités nommées) extraits du texte.\n",
    "    \"\"\"\n",
    "    # Diviser le texte en phrases\n",
    "    sentences = split_text_into_sentences(text)\n",
    "    labels = []\n",
    "\n",
    "    current_chunk = \"\"      # Accumulateur pour stocker les phrases avant de les traiter\n",
    "    chunk_start_offset = 0  # Pour garder une trace du décalage de début du chunk (ensemble de phrases)\n",
    "    \n",
    "    # Utilisation de ThreadPoolExecutor pour traiter les chunks en parallèle\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []        # Liste pour stocker les futures (résultats asynchrones)\n",
    "        for sentence in sentences:\n",
    "            # Si la longueur totale actuelle + celle de la phrase ne dépasse pas 1500 caractères, on l'ajoute au chunk\n",
    "            if len(current_chunk) + len(sentence) + 1 <= 1500:\n",
    "                current_chunk += sentence + \" \"\n",
    "            else:\n",
    "                # Si le chunk dépasse 1500 caractères, traiter le chunk avec le pipeline NER\n",
    "                futures.append(executor.submit(process_chunk, current_chunk, ner_pipeline, chunk_start_offset))\n",
    "                \n",
    "                # Mettre à jour le décalage de départ pour le prochain chunk\n",
    "                chunk_start_offset += len(current_chunk)\n",
    "                \n",
    "                # Réinitialiser le chunk avec la nouvelle phrase\n",
    "                current_chunk = sentence + \" \"\n",
    "\n",
    "        # Si un chunk est encore présent après la boucle, on le traite aussi\n",
    "        if current_chunk:\n",
    "            futures.append(executor.submit(process_chunk, current_chunk, ner_pipeline, chunk_start_offset))\n",
    "        \n",
    "        # Collecter les résultats des tâches parallélisées (futures)\n",
    "        for future in futures:\n",
    "            labels.extend(future.result())\n",
    "\n",
    "    return labels\n",
    "\n",
    "# Fonction pour extraire un mot basé sur un intervalle donné dans le texte\n",
    "def extract_word_by_interval_v2(text, start, end):\n",
    "    \"\"\"\n",
    "    Extrait le mot pointé par l'intervalle donné dans le texte.\n",
    "\n",
    "    Paramètres:\n",
    "    text (str): Le texte d'entrée.\n",
    "    start (int): L'index de début de l'intervalle.\n",
    "    end (int): L'index de fin de l'intervalle.\n",
    "\n",
    "    Retourne:\n",
    "    tuple: Le mot extrait de l'intervalle, l'index de début et l'index de fin mis à jour.\n",
    "    \"\"\"\n",
    "    # Vérifie si les indices sont valides\n",
    "    if start < 0 or end > len(text) or start >= end:\n",
    "        return \"\", start, end\n",
    "\n",
    "    # Ajuster l'index de début à celui du début du mot\n",
    "    word_start = start\n",
    "    while word_start > 0 and re.match(r'[\\w-]', text[word_start - 1]):           # re.match(r'[\\w.-]', text[word_start - 1])\n",
    "        word_start -= 1\n",
    "\n",
    "    # Ajuster l'index de fin à celui de la fin du mot\n",
    "    word_end = end\n",
    "    while word_end < len(text) and re.match(r'[\\w-]', text[word_end]):           # re.match(r'[\\w.-]', text[word_start - 1])\n",
    "        word_end += 1\n",
    "\n",
    "    return text[word_start:word_end], word_start, word_end\n",
    "\n",
    "# Fonction pour corriger les sorties NER en fusionnant les mots séparés et en corrigeant les intervalles\n",
    "def fix_output(NER_output, text):\n",
    "    \"\"\"\n",
    "    Corrige la sortie du NER pour fusionner les mots séparés et ajuster les intervalles.\n",
    "\n",
    "    Paramètres:\n",
    "    NER_output (list): La sortie du modèle NER.\n",
    "    text (str): Le texte original.\n",
    "\n",
    "    Retourne:\n",
    "    list: La sortie NER corrigée.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    pointer = 0\n",
    "    for n in NER_output:\n",
    "        if pointer <= n['start']:\n",
    "            n['word'], n['start'], ending = extract_word_by_interval_v2(text, n['start'], n['end'])\n",
    "            n['end'] = ending\n",
    "            pointer = ending\n",
    "            out.append(n)\n",
    "    return out\n",
    "\n",
    "# Fonction pour diviser le texte en phrases\n",
    "def split_text_into_sentences(text):\n",
    "    \"\"\"\n",
    "    Divise le texte en phrases en utilisant NLTK.\n",
    "\n",
    "    Paramètres:\n",
    "    text (str): Le texte d'entrée.\n",
    "\n",
    "    Retourne:\n",
    "    list: La liste des phrases.\n",
    "    \"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "# Fonction pour traiter un segment de texte à travers le pipeline NER et ajuster les offsets\n",
    "def process_chunk(chunk, ner_pipeline, start_offset):\n",
    "    \"\"\"\n",
    "    Traite un segment de texte à travers le pipeline NER et ajuste les offsets.\n",
    "\n",
    "    Paramètres:\n",
    "    chunk (str): Le segment de texte.\n",
    "    ner_pipeline (pipeline): Le pipeline NER.\n",
    "    start_offset (int): L'offset de début pour ce segment.\n",
    "\n",
    "    Retourne:\n",
    "    list: Les étiquettes NER avec les offsets ajustés.\n",
    "    \"\"\"\n",
    "    chunk_labels = ner_pipeline(chunk)\n",
    "    for label in chunk_labels:\n",
    "        if label['entity_group'] == 'PER':       # Se concentre uniquement sur les entités de type \"PER\"\n",
    "            label['start'] += start_offset\n",
    "            label['end'] += start_offset\n",
    "    return [label for label in chunk_labels if label['entity_group'] == 'PER']\n",
    "\n",
    "# Fonction pour diviser et mettre en minuscules les mots d'une étiquette\n",
    "def split_and_lowercase_v2(label):\n",
    "    \"\"\"\n",
    "    Divise l'étiquette en mots et les convertit en minuscules.\n",
    "\n",
    "    Paramètres:\n",
    "    label (str): L'étiquette à traiter.\n",
    "\n",
    "    Retourne:\n",
    "    list: Une liste de mots en minuscules.\n",
    "    \"\"\"\n",
    "    label = [word.lower() for word in label.split()]\n",
    "    y = []\n",
    "    for x in label:\n",
    "        # Expression régulière pour capturer les mots avec le format 'mot.-mot'\n",
    "        pattern = re.compile(r\"[A-Za-z]+[.]+[-][A-Za-z]+\")\n",
    "\n",
    "        # Appliquer l'expression régulière\n",
    "        match = pattern.match(x)\n",
    "\n",
    "        if match:\n",
    "            y.extend([x])                        # Si la correspondance est trouvée, ajouter le mot\n",
    "        else:\n",
    "            y.extend(re.findall(r'\\b\\w+\\b', x))  # Sinon, diviser les mots par la méthode générique\n",
    "            \n",
    "    return y\n",
    "\n",
    "# Fonction pour ajouter toutes les occurrences des mots dans la chaîne d'origine à partir de la sortie NER\n",
    "def add_all_word_occurrences(ner_output, original_string):\n",
    "    \"\"\"\n",
    "    Ajoute toutes les occurrences de mots dans la chaîne originale en fonction de la sortie NER.\n",
    "\n",
    "    Paramètres:\n",
    "    ner_output (list): La sortie NER.\n",
    "    original_string (str): La chaîne de texte originale.\n",
    "\n",
    "    Retourne:\n",
    "    list: Toutes les occurrences des mots trouvés dans la chaîne originale.\n",
    "    \"\"\"\n",
    "    original_string = original_string.lower()\n",
    "    all_occurrences = []\n",
    "    ner_output = [word for label in ner_output for word in split_and_lowercase_v2(label)]\n",
    "    \n",
    "    for word in ner_output:\n",
    "        start = 0\n",
    "        while start < len(original_string):\n",
    "            start = original_string.find(word, start)\n",
    "            if start == -1:\n",
    "                break\n",
    "            all_occurrences.append(word)\n",
    "            start += len(word)\n",
    "    \n",
    "    return all_occurrences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dee8f0-8d61-4e58-82cc-caddbebdae64",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# La BlackListe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cea0437e-03b4-4fa2-a939-197cad53ce45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Charger la BlackList du fichier\n",
    "with open(\"Anonymisation/blacklist/blacklist.pkl\", \"rb\") as file:  # Use \"rb\" to read in binary mode\n",
    "    blacklist = pickle.load(file)\n",
    "\n",
    "def filter_label(labels, blacklist):\n",
    "    \"\"\"\n",
    "    Éliminer certaines étiquettes identifiées selon une liste noire.\n",
    "\n",
    "    Paramètres:\n",
    "    labels (list): Les mots identifiés.\n",
    "    blacklist (list): La liste noire de mots à exclure.\n",
    "\n",
    "    Retourne:\n",
    "    fixed_labels (list): Les mots identifiés filtrés.\n",
    "    \"\"\"\n",
    "    fixed_labels = []\n",
    "    black_list = blacklist  # Utilisation d'une liste noire pour des recherches rapides\n",
    "    digit_pattern = re.compile(r'\\d')  # Précompiler la regex pour chercher des chiffres\n",
    "    for word in labels:\n",
    "        word_lower = word.lower()\n",
    "        if not digit_pattern.search(word):  # Vérifie s'il n'y a pas de chiffre dans le mot\n",
    "            if word_lower not in black_list:  # Vérifie si le mot est dans la blacklist \n",
    "                fixed_labels.append(word)\n",
    "\n",
    "    return fixed_labels\n",
    "\n",
    "def find_whole_word(original_string, word, start=0):    \n",
    "    \"\"\"\n",
    "    Trouve un mot entier dans une chaîne d'origine.\n",
    "\n",
    "    Paramètres:\n",
    "    original_string (str): La chaîne dans laquelle rechercher le mot.\n",
    "    word (str): Le mot à rechercher.\n",
    "    start (int): L'index à partir duquel commencer la recherche (par défaut à 0).\n",
    "\n",
    "    Retourne:\n",
    "    int: L'index de début du mot s'il est trouvé, -1 sinon.\n",
    "    \"\"\"\n",
    "    # Créer un motif regex qui correspond au mot entier\n",
    "    pattern = r'\\b' + re.escape(word) + r'\\b'\n",
    "    \n",
    "    # Utiliser re.search pour trouver le mot à partir de l'index donné\n",
    "    match = re.search(pattern, original_string[start:])\n",
    "    \n",
    "    if match:\n",
    "        return start + match.start()\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd037ec-61d0-4aee-a1e8-507bfa676dbb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Remplacement de noms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "121c7e51-c22c-4298-8c7e-c6b4d9ad7bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from transformers import pipeline\n",
    "\n",
    "# Télécharge et prépare la base de donnée de noms\n",
    "def load_name_database(file_path):\n",
    "    \"\"\"\n",
    "    Load the name database from a CSV file containing only a single column of names.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): Path to the CSV file containing names.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of names.\n",
    "    \"\"\"\n",
    "    # Load the CSV file\n",
    "    names_df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Check if the 'name' column is present\n",
    "    if 'name' not in names_df.columns:\n",
    "        raise ValueError(\"CSV file must contain a 'name' column.\")\n",
    "    \n",
    "    # Convert the 'name' column to a list\n",
    "    names_list = names_df['name'].tolist()\n",
    "    return names_list\n",
    "\n",
    "# fonction de remplacement de noms dans le texte\n",
    "def change_all_word_occurrences_v2(ner_output, original_string, name_database):\n",
    "    \"\"\"\n",
    "    Changer toutes les occurrences de mots dans la chaîne d'origine en fonction de la sortie NER.\n",
    "\n",
    "    Paramètres:\n",
    "    ner_output (list): La sortie NER, une liste de mots.\n",
    "    original_string (str): La chaîne de texte originale.\n",
    "    name_database (list de str): Liste de noms de remplacements.\n",
    "\n",
    "    Retourne:\n",
    "    list: Toutes les occurrences des mots trouvés dans la chaîne originale.\n",
    "    \"\"\"\n",
    "    original_string = original_string.lower()       # Convertir la chaîne originale en minuscules pour des correspondances insensibles à la casse\n",
    "    modified_string = original_string\n",
    "    all_occurrences = []\n",
    "    changed = []                                    # Liste pour suivre les mots déjà vérifiés\n",
    "    ner_output = [word for label in ner_output for word in split_and_lowercase_v2(label)]\n",
    "    \n",
    "    # Dictionary to store randomized replacements for each detected word\n",
    "    replacements = {}\n",
    "    \n",
    "    for word in ner_output:\n",
    "        if word in changed:\n",
    "            continue\n",
    "        elif len(word) <= 2:                        # Changer pour vérifier les mots changer sont de longueur supérieure à 2\n",
    "            continue\n",
    "        \n",
    "        if word not in replacements:                               #choisi un nom de remplacemement\n",
    "            replacements[word] = random.choice(name_database)\n",
    "            \n",
    "        start = 0\n",
    "        while start < len(modified_string):\n",
    "            start = find_whole_word(original_string, word, start)\n",
    "            if start == -1:\n",
    "                break\n",
    "            end = start + len(word)\n",
    "            modified_string = (\n",
    "                modified_string[:start] + replacements[word] + modified_string[end:]\n",
    "            )\n",
    "            original_string = (\n",
    "                original_string[:start] + replacements[word] + original_string[end:]\n",
    "            )\n",
    "            start += len(replacements[word]) \n",
    "            changed.append(word)       # Ajouter le mot à la liste des mots vérifiés\n",
    "\n",
    "    return modified_string, replacements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf53544-aa85-4ce4-bbd9-0040b9a590b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Modèle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "187db906-c2c8-4830-bc95-87b509a8970d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_packaged_v2(text, ner_pipeline, blacklist, name_database, names=None):\n",
    "    \"\"\"\n",
    "    Fonction principale de traitement de texte avec traduction, transformation et pipeline NER.\n",
    "    \n",
    "    Paramètres:\n",
    "    text (str): Texte à traiter.\n",
    "    ner_pipeline (pipeline): Pipeline NER pour la reconnaissance des entités nommées.\n",
    "    blacklist (list): Liste des termes à exclure.\n",
    "    name_database (list): Base de données des noms pour les remplacements.\n",
    "    names (list of string, optional): Liste de noms supplémentaires à inclure dans le traitement.\n",
    "    \n",
    "    Retourne:\n",
    "    list: Liste des occurrences de mots trouvés dans le texte original.\n",
    "    \"\"\"\n",
    "    # Étape 1 : Traduction du texte\n",
    "    translated = translate_text_v2(text)\n",
    "    \n",
    "    # Étape 2 : Transformation du texte (par ex. mise en forme des majuscules)\n",
    "    transformed = transform_text(translated)\n",
    "    \n",
    "    # Étape 3 : Application du pipeline NER pour extraire les étiquettes\n",
    "    raw_labels = NER_text_split_v2(transformed, ner_pipeline)\n",
    "    \n",
    "    # Étape 4 : Correction des étiquettes NER pour ajuster les mots et les intervalles\n",
    "    ner_output = fix_output(raw_labels, transformed)\n",
    "    \n",
    "    # Extraction des mots de sortie\n",
    "    labels = [y['word'] for y in ner_output]\n",
    "    \n",
    "    # Ajout des noms dans la liste des étiquettes si fourni\n",
    "    if names:\n",
    "        split_names = [name_part for name in names for name_part in split_and_lowercase_v2.split()]\n",
    "        labels.extend(split_names)\n",
    "    \n",
    "    # Filtrage des étiquettes en fonction de la blacklist\n",
    "    fixed_labels = filter_label(labels, blacklist)\n",
    "    \n",
    "    # change de toutes les occurrences de mots dans le texte original\n",
    "    return change_all_word_occurrences_v2(fixed_labels, text, name_database) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214b4304-7664-4600-b623-01abba2f156c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Application du Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e31fd094-eb33-4cdc-9b0e-3a24622148a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, MapType\n",
    "import pandas as pd\n",
    "\n",
    "def apply_model(model, TestSet, ner_pipeline, blacklist, name_database):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    model (function): The decision function (String -> List of String)\n",
    "    TestSet (DataFrame): Test set dataframe containing column 'observation_value'.\n",
    "    ner_pipeline (object): NER pipeline to extract named entities.\n",
    "    blacklist (list de str): Liste de mots à exclure des anonymisation.\n",
    "    name_database (list de str): Liste de noms de remplacements.\n",
    "    \n",
    "    Returns:\n",
    "    Anonymised (DataFrame): Anonymised text dataframe.\n",
    "    \"\"\"\n",
    "    # Récupère le nombre total de lignes dans l'ensemble de test\n",
    "    n = TestSet.count()\n",
    "    \n",
    "    # Listes pour stocker les résultats d'anonymisation   \n",
    "    anonymized_data = []\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"index\", IntegerType(), True),\n",
    "        StructField(\"anonymized_text\", StringType(), True),\n",
    "        StructField(\"name_replacements\",  MapType(StringType(), StringType()), True)\n",
    "    ])\n",
    "    \n",
    "    # Boucle sur chaque ligne de l'ensemble de test\n",
    "    for count in tqdm(range(n), desc=\"Anonymizing Text\", unit=\"row\"):\n",
    "        # Récupère la ligne en fonction de l'index actuel\n",
    "        row = TestSet.filter(col(\"index\") == count).collect()[0]\n",
    "        \n",
    "        # Extraction des observations et des étiquettes réelles de la ligne\n",
    "        x = row['observation_value']\n",
    "        \n",
    "        # Vérifie si la colonne 'name' existe dans la ligne\n",
    "        if 'name' in row:\n",
    "            name = row['name']\n",
    "            # Applique le modèle avec la colonne 'name'\n",
    "            anonymized_text, replacements = model(x, ner_pipeline, blacklist, name_database, name)\n",
    "        else:\n",
    "            # Applique le modèle sans la colonne 'name'\n",
    "            anonymized_text, replacements = model(x, ner_pipeline, blacklist, name_database)\n",
    "        \n",
    "        # Append the anonymized text to the list\n",
    "        anonymized_data.append(Row(index=count, anonymized_text=anonymized_text, name_replacements=replacements))\n",
    "        \n",
    "    df = pd.DataFrame(anonym, columns=[\"index\", \"observation_value\", \"remplacements\"])\n",
    "    # Sauvegarder dans un fichier CSV\n",
    "    df.to_csv(\"output.csv\", index=False)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762200e0-9d22-4ba9-abfc-485b623e9d3f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd76ede5-cbc3-4b3f-baba-1a9636492c9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+------+\n",
      "|index|   observation_value|               label|N_mots|\n",
      "+-----+--------------------+--------------------+------+\n",
      "|    0|Dossier:  0323480...|[ELARICH, IHSENE,...|   248|\n",
      "|    1|Dossier:  0310728...|[AMMAR, YANNI, SA...|   155|\n",
      "|    2|Dossier:  0239248...|[DIALLO, AISSATOU...|   223|\n",
      "|    3|Dossier:  P011456...|[LAMONTAGNE, MYLE...|   206|\n",
      "|    4|Dossier:  0127689...|[GAGNON, NATACHA,...|   384|\n",
      "|    5|Dossier:  X361189...|[GU, SARAH, Mucha...|   339|\n",
      "|    6|Dossier:  0254246...|[BOUCHARD, SOPHIA...|   424|\n",
      "|    7|Dossier:  0231956...|[LAPORTE, CHRISTO...|   300|\n",
      "|    8|Dossier:  0323811...|[APETOFIA, MATHEO...|   271|\n",
      "|    9|Dossier:  0343202...|[GUTIERREZ MARTIN...|   842|\n",
      "|   10|Dossier:  0219196...|[BENOIT, PIER-OLI...|  1076|\n",
      "|   11|Dossier:  X361387...|[LEBEL, PIERRE, M...|   351|\n",
      "|   12|Dossier:  X361533...|[JOURDAIN, GERARD...|   548|\n",
      "|   13|Dossier:  0330779...|[BAREZI, NICOLE, ...|   735|\n",
      "|   14|Dossier:  X360424...|[NAGRA, GURPREET ...|   348|\n",
      "|   15|Dossier:  0306715...|[BOUDAREL, ELISA,...|   554|\n",
      "|   16|Dossier:  X360375...|[LACHANCE, MIA, D...|   459|\n",
      "|   17|Dossier:  0326528...|[FOURNIER, ANTOIN...|  1382|\n",
      "|   18|Dossier:  0340727...|[LEHR, CLARA, B.-...|   181|\n",
      "|   19|Dossier:  0244368...|[COULOMBE, WILLIA...|   310|\n",
      "+-----+--------------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ExampleApp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Définition de l'ensemble de test à utiliser pour l'évaluation du modèle\n",
    "df = spark.read.parquet('TestSet2.2')  #changer pour l<ensemble de donnée voulu\n",
    "df.show()     #montrer quelque info sur le df\n",
    "df.count()    #montrer quelque info sur le df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe429576-51c5-47a7-9c30-0e903d62ff63",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57be9499-3147-4250-9d8b-3528c3fc5ef8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "################################################### TQDM ########################################################################\n",
    "import re\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "\n",
    "# Chargement du modèle BERT pour la reconnaissance d'entités nommées (NER)\n",
    "loaded_model = AutoModelForTokenClassification.from_pretrained('Anonymisation/bert-large-NER')\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained('Anonymisation/bert-large-NER')\n",
    "\n",
    "# Pipeline NER avec stratégie d'agrégation 'simple'\n",
    "ner_pipeline = pipeline(\"ner\", model=loaded_model, tokenizer=loaded_tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Download la Blackliste\n",
    "with open(\"Anonymisation/blacklist/blacklist.pkl\", \"rb\") as file:  # Use \"rb\" to read in binary mode\n",
    "    blacklist = pickle.load(file)\n",
    "\n",
    "# Download les noms de remplacement\n",
    "name_database = load_name_database('Anonymisation/prenomBD/prenom_M_et_F.csv')\n",
    "\n",
    "# Application du modèle sur l'ensemble de test\n",
    "###anonym = apply_model(model_packaged_v2, TestSet, ner_pipeline, blacklist, name_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40f83a1d-8d75-4424-88ff-5f90e093c1e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+------+\n",
      "|index|   observation_value|               label|N_mots|\n",
      "+-----+--------------------+--------------------+------+\n",
      "|    0|Dossier:  0323480...|[ELARICH, IHSENE,...|   248|\n",
      "|    1|Dossier:  0310728...|[AMMAR, YANNI, SA...|   155|\n",
      "|    2|Dossier:  0239248...|[DIALLO, AISSATOU...|   223|\n",
      "|    3|Dossier:  P011456...|[LAMONTAGNE, MYLE...|   206|\n",
      "|    4|Dossier:  0127689...|[GAGNON, NATACHA,...|   384|\n",
      "+-----+--------------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Limit the DataFrame to the first 5 rows\n",
    "first_five_rows = df.limit(5)\n",
    "\n",
    "# Show the result\n",
    "first_five_rows.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58a7071a-6905-4346-9923-e252ccd730ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Anonymizing Text: 100%|██████████| 5/5 [00:18<00:00,  3.70s/row]\n"
     ]
    }
   ],
   "source": [
    "# Exemple d'ensemble de test et évaluation du modèle\n",
    "TestSet = df.limit(5)  # Chargez votre ensemble de test ici (remplacer df par le DataFrame correct)\n",
    "\n",
    "# Évaluation du modèle sur l'ensemble de test\n",
    "anonym = apply_model(model_packaged_v2, TestSet, ner_pipeline, blacklist, name_database)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb9a437a-8c86-4229-87a5-b62ad961b621",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(anonym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f539685f-cee9-4b63-b36f-c3382f61f8c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.limit(20).write.parquet(\"ExempleSet20.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
